{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01db79bb-d8f7-4c46-8f3b-5bac83e0ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:31,028 - pyswarms.single.global_best - INFO - Optimize for 5 iters with {'c1': 1.5, 'c2': 1.5, 'w': 0.7}\n",
      "pyswarms.single.global_best:   0%|                                                                                              |0/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:31,305 - tensorflow - WARNING - From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:32,169 - tensorflow - WARNING - From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:33,050 - tensorflow - WARNING - From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018E12247B00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:47,798 - tensorflow - WARNING - 5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018E12247B00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018E10F67240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 10:15:50,756 - tensorflow - WARNING - 6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018E10F67240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "pyswarms.single.global_best: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|5/5, best_cost=0.125\n",
      "2026-01-03 10:17:38,647 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.1246049553155899, best pos: [74.61838452  0.27197388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0744 - val_loss: 0.0027\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0667 - val_loss: 0.0026\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0586 - val_loss: 0.0057\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0529 - val_loss: 0.0111\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0611 - val_loss: 0.0188\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0551 - val_loss: 0.0270\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0525 - val_loss: 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 873ms/step\n",
      "\n",
      "‚úÖ DROPOUT RISK MODEL TRAINED SUCCESSFULLY\n",
      "üìÅ Outputs saved in: C:\\Users\\NXTWAVE\\Downloads\\Student Dropout Risk Prediction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import pyswarms as ps\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Student Dropout Risk Prediction\"\n",
    "DATA_PATH = r\"C:\\Users\\NXTWAVE\\Downloads\\Student Dropout Risk Prediction\\RS_Session_254_AU_352.B.csv\"\n",
    "\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "ARTIFACT_DIR = os.path.join(BASE_DIR, \"artifacts\")\n",
    "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Drop non-numeric identifier columns\n",
    "df = df.drop(columns=[\"sl. no.\", \"state/ut\"], errors=\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# SAFETY: REPLACE ZERO WITH NaN (to avoid division by zero)\n",
    "# ============================================================\n",
    "df.replace(0, np.nan, inplace=True)\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DROPOUT RISK (SAFE CALCULATION)\n",
    "# ============================================================\n",
    "df[\"drop_primary_to_upper\"] = (\n",
    "    df[\"primary level (i - v) - total\"] -\n",
    "    df[\"upper primary - total\"]\n",
    ") / df[\"primary level (i - v) - total\"]\n",
    "\n",
    "df[\"drop_upper_to_secondary\"] = (\n",
    "    df[\"upper primary - total\"] -\n",
    "    df[\"secondary (ix-x) - total\"]\n",
    ") / df[\"upper primary - total\"]\n",
    "\n",
    "df[\"drop_secondary_to_higher\"] = (\n",
    "    df[\"secondary (ix-x) - total\"] -\n",
    "    df[\"higher secondary - total\"]\n",
    ") / df[\"secondary (ix-x) - total\"]\n",
    "\n",
    "# ============================================================\n",
    "# FINAL DROPOUT RISK SCORE (0‚Äì1)\n",
    "# ============================================================\n",
    "df[\"dropout_risk\"] = df[\n",
    "    [\n",
    "        \"drop_primary_to_upper\",\n",
    "        \"drop_upper_to_secondary\",\n",
    "        \"drop_secondary_to_higher\"\n",
    "    ]\n",
    "].mean(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN NaN & INF VALUES\n",
    "# ============================================================\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill NaN with column median (best practice)\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Clip risk into valid range\n",
    "df[\"dropout_risk\"] = df[\"dropout_risk\"].clip(0, 1)\n",
    "\n",
    "# ============================================================\n",
    "# FEATURES & TARGET\n",
    "# ============================================================\n",
    "X = df.drop(columns=[\"dropout_risk\"])\n",
    "y = df[\"dropout_risk\"]\n",
    "\n",
    "# ============================================================\n",
    "# SCALING (NOW SAFE)\n",
    "# ============================================================\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# ============================================================\n",
    "# LSTM SHAPE\n",
    "# ============================================================\n",
    "X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_lstm, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# LSTM MODEL\n",
    "# ============================================================\n",
    "def build_lstm(units, dropout):\n",
    "    model = Sequential([\n",
    "        LSTM(int(units), input_shape=(1, X_train.shape[2])),\n",
    "        Dropout(dropout),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# PSO OBJECTIVE FUNCTION\n",
    "# ============================================================\n",
    "def pso_objective(particles):\n",
    "    losses = []\n",
    "\n",
    "    for p in particles:\n",
    "        units = int(p[0])\n",
    "        dropout = p[1]\n",
    "\n",
    "        model = build_lstm(units, dropout)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.array(losses)\n",
    "\n",
    "# ============================================================\n",
    "# PSO OPTIMIZATION\n",
    "# ============================================================\n",
    "bounds = (\n",
    "    np.array([32, 0.1]),\n",
    "    np.array([128, 0.5])\n",
    ")\n",
    "\n",
    "optimizer = ps.single.GlobalBestPSO(\n",
    "    n_particles=8,\n",
    "    dimensions=2,\n",
    "    options={\"c1\": 1.5, \"c2\": 1.5, \"w\": 0.7},\n",
    "    bounds=bounds\n",
    ")\n",
    "\n",
    "best_loss, best_pos = optimizer.optimize(pso_objective, iters=5)\n",
    "\n",
    "best_units = int(best_pos[0])\n",
    "best_dropout = float(best_pos[1])\n",
    "\n",
    "# ============================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# ============================================================\n",
    "final_model = build_lstm(best_units, best_dropout)\n",
    "\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_model.save(os.path.join(MODEL_DIR, \"dropout_lstm_pso.h5\"))\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION\n",
    "# ============================================================\n",
    "y_pred = final_model.predict(X_test).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "results = {\n",
    "    \"mse\": float(mse),\n",
    "    \"r2_score\": float(r2),\n",
    "    \"best_units\": best_units,\n",
    "    \"best_dropout\": best_dropout\n",
    "}\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "with open(os.path.join(ARTIFACT_DIR, \"config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(results, f)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"actual_risk\": y_test.values,\n",
    "    \"predicted_risk\": y_pred\n",
    "}).to_csv(os.path.join(LOG_DIR, \"predictions.csv\"), index=False)\n",
    "\n",
    "print(\"\\n‚úÖ DROPOUT RISK MODEL TRAINED SUCCESSFULLY\")\n",
    "print(\"üìÅ Outputs saved in:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ed34e-d7fe-4c49-8461-1e66c3991da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
